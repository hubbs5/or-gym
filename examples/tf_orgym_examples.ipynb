{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2244831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import or_gym\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc47ced5",
   "metadata": {},
   "source": [
    "# Binary Knapsack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118ab57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = {'N': 5,\n",
    "              'max_weight': 15,\n",
    "              'item_weights': np.array([1, 12, 2, 1, 4]),\n",
    "              'item_values': np.array([2, 4, 2, 1, 10]),\n",
    "              'mask': False}\n",
    "env = or_gym.make('Knapsack-v0', env_config=env_config)  \n",
    "initial_state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05f8c49",
   "metadata": {},
   "source": [
    "The state variable must be read as the following:\n",
    "\n",
    "    Observation:\n",
    "        Type: Tuple, Discrete\n",
    "        0: list of item weights\n",
    "        1: list of item values\n",
    "        2: maximum weight of the knapsack\n",
    "        3: current weight in knapsack\n",
    "\n",
    "    Actions:\n",
    "        Type: Discrete\n",
    "        0: Place item 0 into knapsack\n",
    "        1: Place item 1 into knapsack\n",
    "        2: ...\n",
    "\n",
    "    Reward:\n",
    "        Value of item successfully placed into knapsack or 0 if the item\n",
    "        doesn't fit, at which point the episode ends.\n",
    "\n",
    "    Starting State:\n",
    "        Lists of available items and empty knapsack.\n",
    "\n",
    "    Episode Termination:\n",
    "        Full knapsack or selection that puts the knapsack over the limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70567316",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = env.action_space.n\n",
    "states = env.observation_space\n",
    "states.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fd9311",
   "metadata": {},
   "source": [
    "Simulate random item selection for 10 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c8bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "episode = 0\n",
    "done = False\n",
    "while not done :\n",
    "    episode += 1\n",
    "    print(\"Episode: \" + str(episode))\n",
    "    action = np.random.randint(actions)\n",
    "    print(\"Take element number: \" + str(action))\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    print(\"Reward: \" + str(reward))\n",
    "    print(next_state)\n",
    "    print(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7375ad77",
   "metadata": {},
   "source": [
    "As we can see in the detailed print out of the observation space, it is just the last index value which changes from episode to episode. This index is equal to the current total weight of the knapsack. The observation space from the environment gives no indication on the total value collected, which is instead added by the render() function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4260a48b",
   "metadata": {},
   "source": [
    "# Keras model for the knapsack decision environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d5ba79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()  \n",
    "model.add(Dense(24, activation='relu', input_shape=states.shape))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(actions, activation='linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e842c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].get_input_shape_at(0) # get the input shape of desired layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f787d2",
   "metadata": {},
   "source": [
    "# Agent training with Keras RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e0907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = BoltzmannQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
