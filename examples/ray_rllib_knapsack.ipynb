{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973f2613",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib import agents\n",
    "from ray import tune\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.models.tf.fcnet import FullyConnectedNetwork\n",
    "from ray.rllib.utils import try_import_tf\n",
    "from gym import spaces\n",
    "import or_gym\n",
    "from or_gym.utils import create_env\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4201d91c",
   "metadata": {},
   "source": [
    "# Prepare Tensforflow and ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac8ec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish Tensorflow API conncetion\n",
    "tf_api, tf_original, tf_version = try_import_tf(error = True) \n",
    "# Disable callback synch on Windows\n",
    "TUNE_DISABLE_AUTO_CALLBACK_SYNCER=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acd05db",
   "metadata": {},
   "source": [
    "# Knapsack environment with action masking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bc4f2b",
   "metadata": {},
   "source": [
    "Class definition: customized Tensorflow-2-model for OR-Gym knapsack envrionemnt with action masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b629ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KP0ActionMaskModel(TFModelV2):\n",
    "     \n",
    "    def __init__(self, obs_space, action_space, num_outputs,\n",
    "        model_config, name, true_obs_shape=(11,),\n",
    "        action_embed_size=5, *args, **kwargs):\n",
    "        \n",
    "        # true_obs_shape is going to match the size of the state. \n",
    "        # If we stick with our reduced KP, that will be a vector with 11 entries. \n",
    "        # The other value we need to provide is the action_embed_size, which is going to be the size of our action space (5)\n",
    "         \n",
    "        super(KP0ActionMaskModel, self).__init__(obs_space,\n",
    "            action_space, num_outputs, model_config, name, \n",
    "            *args, **kwargs)\n",
    "         \n",
    "        self.action_embed_model = FullyConnectedNetwork(\n",
    "            spaces.Box(0, 1, shape=true_obs_shape), \n",
    "                action_space, action_embed_size,\n",
    "            model_config, name + \"_action_embedding\")\n",
    "        self.register_variables(self.action_embed_model.variables())\n",
    " \n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        \n",
    "        # The actual masking takes place in the forward method where we unpack the mask, actions, and state from \n",
    "        # the observation dictionary provided by our environment. The state yields our action embeddings which gets \n",
    "        # combined with our mask to provide logits with the smallest value we can provide. \n",
    "        # This will get passed to a softmax output which will reduce the probability of selecting these actions to 0, \n",
    "        # effectively blocking the agent from ever taking these illegal actions.\n",
    "        \n",
    "        avail_actions = input_dict[\"obs\"][\"avail_actions\"]\n",
    "        action_mask = input_dict[\"obs\"][\"action_mask\"]\n",
    "        action_embedding, _ = self.action_embed_model({\n",
    "            \"obs\": input_dict[\"obs\"][\"state\"]})\n",
    "        intent_vector = tf_api.expand_dims(action_embedding, 1)\n",
    "        action_logits = tf_api.reduce_sum(avail_actions * intent_vector, axis=1)\n",
    "        inf_mask = tf_api.maximum(tf_api.log(action_mask), tf_api.float32.min)\n",
    "        return action_logits + inf_mask, state\n",
    " \n",
    "    def value_function(self):\n",
    "        return self.action_embed_model.value_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4246a847",
   "metadata": {},
   "source": [
    "Environment creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d15617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for gym environment\n",
    "env_config = {'N': 5,\n",
    "              'max_weight': 15,\n",
    "              'item_weights': np.array([1, 12, 2, 1, 4]),\n",
    "              'item_values': np.array([2, 4, 2, 1, 10]),\n",
    "              'mask': True}\n",
    " \n",
    "env_name = 'Knapsack-v0'\n",
    "env = or_gym.make('Knapsack-v0', env_config=env_config)\n",
    " \n",
    "print(\"Max weight capacity:\\t{}kg\".format(env.max_weight))\n",
    "print(\"Number of items:\\t{}\".format(env.N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3710cae",
   "metadata": {},
   "source": [
    "Create Rllib trainable instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78cf89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the model for Rllib usage\n",
    "ModelCatalog.register_custom_model('kp_mask', KP0ActionMaskModel)\n",
    "# Register the environment, so that we have a Trainable instance later\n",
    "# ATTENTION: Tune needs the base class, not an instance of the environment like we get from or_gym.make(env_name) to work with. So we need to pass this to register_env using a lambda function as shown below.\n",
    "env = create_env(env_name)\n",
    "tune.register_env(env_name, lambda env_name: env(env_name, env_config=env_config))\n",
    "\n",
    "trainer_config = {\n",
    "    \"model\": {\n",
    "        \"custom_model\": \"kp_mask\"   # Here we must use the custom model name taken in register process before\n",
    "        },\n",
    "    \"env_config\": env_config,       # env config from (or_)gym\n",
    "    #\"framework\" : \"tfe\"             # tip by rllib to enable TensorFlow eager exection\n",
    "     }\n",
    "\n",
    "# ray.shutdown() maybe necessary in case of blocking instance\n",
    "ray.init( ignore_reinit_error = True )\n",
    "trainer = agents.ppo.PPOTrainer(env='Knapsack-v0', config=trainer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e87dff",
   "metadata": {},
   "source": [
    "Train the agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da600b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = trainer.env_creator('Knapsack-v0')\n",
    "state = env.state\n",
    "\n",
    "# Use the action masking to disable the agent to take specific actions, i.e. to avoid taking element in knapsack by index\n",
    "# state['action_mask'][0] = 0\n",
    "\n",
    "# Train an agent for 1000 states \n",
    "actions = np.array([trainer.compute_single_action(state) for i in range(10000)])\n",
    "\n",
    "# If action masking used, check that this action was never taken\n",
    "# print(any(actions==0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8848f472",
   "metadata": {},
   "source": [
    "# Tuning hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07a0cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tune for hyperparameter tuning\n",
    "tune_config = {\n",
    "    'env': 'Knapsack-v0'\n",
    "}\n",
    "stop = {\n",
    "    'timesteps_total': 10000\n",
    "}\n",
    "results = tune.run(\n",
    "    'PPO', # Specify the algorithm to train\n",
    "    config=tune_config,\n",
    "    stop=stop\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e31e7ab",
   "metadata": {},
   "source": [
    "Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817660cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "df = results.dataframe()\n",
    "# Get column for total loss, policy loss, and value loss\n",
    "tl_col = [i for i, j in enumerate(df.columns)\n",
    "          if 'total_loss' in j][0]\n",
    "pl_col = [i for i, j in enumerate(df.columns)\n",
    "          if 'policy_loss' in j][0]\n",
    "vl_col = [i for i, j in enumerate(df.columns)\n",
    "          if 'vf_loss' in j][0]\n",
    "labels = []\n",
    "fig, ax = plt.subplots(2, 2, figsize=(15, 15), sharex=True)\n",
    "for i, path in df['logdir'].iteritems():\n",
    "    data = pd.read_csv(path + '/progress.csv')\n",
    "    # Get labels for legend\n",
    "    lr = data['experiment_id'][0]\n",
    "    layers = data['training_iteration'][0]\n",
    "    labels.append('LR={}; Shared Layers={}'.format(lr, layers))\n",
    "     \n",
    "    ax[0, 0].plot(data['timesteps_total'], \n",
    "            data['episode_reward_mean'], c=colors[i],\n",
    "            label=labels[-1])\n",
    "     \n",
    "    ax[0, 1].plot(data['timesteps_total'], \n",
    "           data.iloc[:, tl_col], c=colors[i],\n",
    "           label=labels[-1])\n",
    "     \n",
    "    ax[1, 0].plot(data['timesteps_total'], \n",
    "               data.iloc[:, pl_col], c=colors[i],\n",
    "               label=labels[-1])\n",
    "     \n",
    "     \n",
    "    ax[1, 1].plot(data['timesteps_total'], \n",
    "               data.iloc[:, vl_col], c=colors[i],\n",
    "               label=labels[-1])\n",
    " \n",
    "ax[0, 0].set_ylabel('Mean Rewards')\n",
    "ax[0, 0].set_title('Training Rewards by Time Step')\n",
    "ax[0, 0].legend(labels=labels, loc='upper center',\n",
    "        ncol=3, bbox_to_anchor=[0.75, 1.2])\n",
    " \n",
    " \n",
    "ax[0, 1].set_title('Total Loss by Time Step')\n",
    "ax[0, 1].set_ylabel('Total Loss')\n",
    "ax[0, 1].set_xlabel('Training Episodes')\n",
    " \n",
    "ax[1, 0].set_title('Policy Loss by Time Step')\n",
    "ax[1, 0].set_ylabel('Policy Loss')\n",
    "ax[1, 0].set_xlabel('Time Step')\n",
    " \n",
    "ax[1, 1].set_title('Value Loss by Time Step')\n",
    "ax[1, 1].set_ylabel('Value Loss')\n",
    "ax[1, 1].set_xlabel('Time Step')\n",
    " \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "or-gym",
   "language": "python",
   "name": "or-gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
