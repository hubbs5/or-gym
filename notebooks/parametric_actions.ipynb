{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ray\n",
    "from ray import tune\n",
    "import gym\n",
    "from gym import spaces\n",
    "from or_gym.utils.env_config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to enforce constraints on the model to restrict actions in certain cases. Here, I'll implement a simple knapsack environment to limit the algorithm from selecting items that cause it to exceed its limit. There will be three actions available to the algorithm.\n",
    "\n",
    "- 0: end episode\n",
    "- 1: accept item\n",
    "- 2: reject item\n",
    "\n",
    "If 0 is selected, the episode ends and the agent collects no additional reward. If 1 is selected, the agent packs that item and collects the reward. If 2 is selected, the agent rejects the item and moves to the next. \n",
    "\n",
    "If the parametric action selection works properly, the agent should never exceed the capacity of the knapsack and receive a large, negative reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParametricKnapsack(gym.Env):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.step_limit = 10\n",
    "        self.item_values = np.random.randint(0, 10, self.step_limit)\n",
    "        self.item_weights = np.random.randint(1, 5, self.step_limit)\n",
    "        self.weight_capacity = 20\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.mask = True\n",
    "        assign_env_config(self, kwargs)\n",
    "        if self.mask:\n",
    "            self.observation_space = spaces.Dict({\n",
    "                \"action_mask\": spaces.Box(0, 1, shape=(3,)),\n",
    "                \"avail_actions\": spaces.Box(0, 1, shape=(3,)),\n",
    "                \"knapsack\": spaces.Box(0, self.weight_capacity, shape=(3,))\n",
    "            })\n",
    "        else:\n",
    "            self.observation_space = spaces.Dict({\n",
    "                \"knapsack\": spaces.Box(0, self.weight_capacity, shape=(3,))\n",
    "            })\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_weight, self.current_step = 0, 0\n",
    "        self.item_values = np.random.randint(0, 10, self.step_limit)\n",
    "        self.item_weights = np.random.randint(1, 5, self.step_limit)\n",
    "        if self.mask:\n",
    "            self.state = {\n",
    "                \"action_mask\": np.ones(3),\n",
    "                \"avail_actions\": np.ones(3),\n",
    "                \"knapsack\": np.array(\n",
    "                    [self.current_weight, \n",
    "                     self.item_values[self.current_step], \n",
    "                     self.item_weights[self.current_step]])}\n",
    "        else:\n",
    "            self.state = {\"knapsack\": np.array(\n",
    "                    [self.current_weight, \n",
    "                     self.item_values[self.current_step], \n",
    "                     self.item_weights[self.current_step]])}\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.current_weight = self.state[\"knapsack\"][0]\n",
    "        item_value = self.state[\"knapsack\"][1]\n",
    "        item_weight = self.state[\"knapsack\"][2]\n",
    "        done = False\n",
    "        if action == 0:\n",
    "            # End episode\n",
    "            done = True\n",
    "            reward = 0\n",
    "        elif action == 1:\n",
    "            # Accept item\n",
    "            if self.current_weight + item_weight <= self.weight_capacity:\n",
    "                self.current_weight += item_weight\n",
    "                reward = item_value\n",
    "                # End if capacity is met\n",
    "                if self.current_weight == self.weight_capacity:\n",
    "                    done = True\n",
    "            else: # Overweight\n",
    "                reward = -100\n",
    "                done = True\n",
    "        elif action == 2:\n",
    "            # Reject item\n",
    "            reward = 0\n",
    "        \n",
    "        self.current_step += 1\n",
    "        if self.current_step >= self.step_limit:\n",
    "            done = True\n",
    "        self.update_state()\n",
    "        return self.state, reward, done, {}\n",
    "    \n",
    "    def update_state(self):\n",
    "        # Make action selection impossible if the knapsack would go over weight\n",
    "        step = self.current_step if self.current_step < self.step_limit else self.step_limit-1\n",
    "        knapsack = np.array([self.current_weight, \n",
    "                self.item_values[step], \n",
    "                self.item_weights[step]])\n",
    "        if self.current_weight + knapsack[-1] > self.weight_capacity:\n",
    "            action_mask = np.array([1, 0, 1])\n",
    "        else:\n",
    "            action_mask = np.ones(3)\n",
    "        if self.mask:\n",
    "            self.state = {\n",
    "                \"action_mask\": action_mask,\n",
    "                \"avail_actions\": np.ones(3),\n",
    "                \"knapsack\": knapsack\n",
    "            }\n",
    "        else:\n",
    "            self.state = {\"knapsack\": knapsack}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ParametricKnapsack(env_config={\"mask\": True})\n",
    "rewards, steps = [], []\n",
    "\n",
    "for i in range(100):\n",
    "    env.reset()\n",
    "    reward = 0\n",
    "    done = False\n",
    "    while done == False:\n",
    "        action = env.action_space.sample()\n",
    "        s, r, done, _ = env.step(action)\n",
    "        reward += r\n",
    "        if done:\n",
    "            rewards.append(reward)\n",
    "            steps.append(env.current_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action_mask': array([1., 1., 1.]),\n",
       " 'avail_actions': array([1., 1., 1.]),\n",
       " 'knapsack': array([0, 5, 4])}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'action_mask': array([1, 0, 1]),\n",
       "  'avail_actions': array([1., 1., 1.]),\n",
       "  'knapsack': array([20,  8,  2])},\n",
       " 2,\n",
       " True)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s, r, done, _ = env.step(1)\n",
    "s, r, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('knapsack',\n",
       "              array([ 6.8426876, 11.413433 , 18.437813 ], dtype=float32))])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env(config_env):\n",
    "    return ParametricKnapsack()\n",
    "\n",
    "tune.register_env(\"ParaKnapsack-v0\", lambda config: create_env(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-21 17:17:34,873\tWARNING services.py:597 -- setpgrp failed, processes may not be cleaned up properly: [Errno 1] Operation not permitted.\n",
      "2020-04-21 17:17:34,878\tINFO resource_spec.py:216 -- Starting Ray with 2.98 GiB memory available for workers and up to 1.5 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.11',\n",
       " 'redis_address': '192.168.0.11:47587',\n",
       " 'object_store_address': '/tmp/ray/session_2020-04-21_17-17-34_872332_30574/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-04-21_17-17-34_872332_30574/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2020-04-21_17-17-34_872332_30574'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-21 15:51:23,806\tINFO ray_trial_executor.py:121 -- Trial PPO_ParaKnapsack-v0_dc90e01c: Setting up new remote runner.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 5.1/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/8 CPUs, 0/1 GPUs, 0.0/3.17 GiB heap, 0.0/1.07 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ParaKnapsack-v0_dc90e01c</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=27221)\u001b[0m 2020-04-21 15:51:25,909\tINFO trainer.py:371 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=27221)\u001b[0m 2020-04-21 15:51:25,912\tINFO trainer.py:512 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=27221)\u001b[0m 2020-04-21 15:51:28,196\tWARNING util.py:45 -- Install gputil for GPU system monitoring.\n",
      "Result for PPO_ParaKnapsack-v0_dc90e01c:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-04-21_15-51-34\n",
      "  done: false\n",
      "  episode_len_mean: 2.9469026548672566\n",
      "  episode_reward_max: 45.0\n",
      "  episode_reward_mean: 4.153392330383481\n",
      "  episode_reward_min: -80.0\n",
      "  episodes_this_iter: 1356\n",
      "  episodes_total: 1356\n",
      "  experiment_id: b1c32acb32ee4cbb9559a4c4cbe3d7d5\n",
      "  experiment_tag: '0'\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 1871.544\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0702439546585083\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.030406463891267776\n",
      "        policy_loss: -0.08876977860927582\n",
      "        total_loss: 79.7486801147461\n",
      "        vf_explained_var: 0.018759524449706078\n",
      "        vf_loss: 79.83135986328125\n",
      "    load_time_ms: 44.142\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 3968\n",
      "    sample_time_ms: 3415.185\n",
      "    update_time_ms: 437.565\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.04444444444445\n",
      "    ram_util_percent: 38.49999999999999\n",
      "  pid: 27221\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.050231760674826875\n",
      "    mean_inference_ms: 0.5532347867010924\n",
      "    mean_processing_ms: 0.29033237998919453\n",
      "  time_since_restore: 5.813212156295776\n",
      "  time_this_iter_s: 5.813212156295776\n",
      "  time_total_s: 5.813212156295776\n",
      "  timestamp: 1587502294\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: dc90e01c\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 6.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/8 CPUs, 0/1 GPUs, 0.0/3.17 GiB heap, 0.0/1.07 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ParaKnapsack-v0_dc90e01c</td><td>RUNNING </td><td>192.168.0.11:27221</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.81321</td><td style=\"text-align: right;\">       4000</td><td style=\"text-align: right;\"> 4.15339</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ParaKnapsack-v0_dc90e01c:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-04-21_15-51-40\n",
      "  done: false\n",
      "  episode_len_mean: 5.056746532156368\n",
      "  episode_reward_max: 48.0\n",
      "  episode_reward_mean: 11.94577553593947\n",
      "  episode_reward_min: -83.0\n",
      "  episodes_this_iter: 793\n",
      "  episodes_total: 3173\n",
      "  experiment_id: b1c32acb32ee4cbb9559a4c4cbe3d7d5\n",
      "  experiment_tag: '0'\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 1698.42\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8514610528945923\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03969983384013176\n",
      "        policy_loss: -0.08992589265108109\n",
      "        total_loss: 99.16346740722656\n",
      "        vf_explained_var: 0.10799845308065414\n",
      "        vf_loss: 99.23554229736328\n",
      "    load_time_ms: 15.45\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 11904\n",
      "    sample_time_ms: 2227.719\n",
      "    update_time_ms: 147.567\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.95\n",
      "    ram_util_percent: 38.8\n",
      "  pid: 27221\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.04929827295109617\n",
      "    mean_inference_ms: 0.5321543659855011\n",
      "    mean_processing_ms: 0.24867016477284373\n",
      "  time_since_restore: 12.334480047225952\n",
      "  time_this_iter_s: 3.2525575160980225\n",
      "  time_total_s: 12.334480047225952\n",
      "  timestamp: 1587502300\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: dc90e01c\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 6.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/8 CPUs, 0/1 GPUs, 0.0/3.17 GiB heap, 0.0/1.07 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ParaKnapsack-v0_dc90e01c</td><td>RUNNING </td><td>192.168.0.11:27221</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         12.3345</td><td style=\"text-align: right;\">      12000</td><td style=\"text-align: right;\"> 11.9458</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ParaKnapsack-v0_dc90e01c:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-04-21_15-51-47\n",
      "  done: false\n",
      "  episode_len_mean: 6.947916666666667\n",
      "  episode_reward_max: 58.0\n",
      "  episode_reward_mean: 21.432291666666668\n",
      "  episode_reward_min: -76.0\n",
      "  episodes_this_iter: 576\n",
      "  episodes_total: 4426\n",
      "  experiment_id: b1c32acb32ee4cbb9559a4c4cbe3d7d5\n",
      "  experiment_tag: '0'\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 1665.247\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6899988055229187\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012222486548125744\n",
      "        policy_loss: -0.041779011487960815\n",
      "        total_loss: 155.94522094726562\n",
      "        vf_explained_var: 0.23325222730636597\n",
      "        vf_loss: 155.97872924804688\n",
      "    load_time_ms: 9.723\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 19840\n",
      "    sample_time_ms: 1965.186\n",
      "    update_time_ms: 89.762\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.55\n",
      "    ram_util_percent: 38.8\n",
      "  pid: 27221\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.049497884516715523\n",
      "    mean_inference_ms: 0.5290987600136872\n",
      "    mean_processing_ms: 0.22841991372268666\n",
      "  time_since_restore: 18.733412981033325\n",
      "  time_this_iter_s: 3.198996067047119\n",
      "  time_total_s: 18.733412981033325\n",
      "  timestamp: 1587502307\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: dc90e01c\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 6.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/8 CPUs, 0/1 GPUs, 0.0/3.17 GiB heap, 0.0/1.07 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ParaKnapsack-v0_dc90e01c</td><td>RUNNING </td><td>192.168.0.11:27221</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         18.7334</td><td style=\"text-align: right;\">      20000</td><td style=\"text-align: right;\"> 21.4323</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ParaKnapsack-v0_dc90e01c:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-04-21_15-51-53\n",
      "  done: false\n",
      "  episode_len_mean: 8.123481781376519\n",
      "  episode_reward_max: 64.0\n",
      "  episode_reward_mean: 29.251012145748987\n",
      "  episode_reward_min: -61.0\n",
      "  episodes_this_iter: 494\n",
      "  episodes_total: 5443\n",
      "  experiment_id: b1c32acb32ee4cbb9559a4c4cbe3d7d5\n",
      "  experiment_tag: '0'\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 1652.311\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5546323657035828\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006457101088017225\n",
      "        policy_loss: -0.024905499070882797\n",
      "        total_loss: 127.29461669921875\n",
      "        vf_explained_var: 0.40242043137550354\n",
      "        vf_loss: 127.31517028808594\n",
      "    load_time_ms: 7.276\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 27776\n",
      "    sample_time_ms: 1848.378\n",
      "    update_time_ms: 64.859\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.3\n",
      "    ram_util_percent: 38.975\n",
      "  pid: 27221\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.04977444695192232\n",
      "    mean_inference_ms: 0.5291254706690051\n",
      "    mean_processing_ms: 0.2166286511607867\n",
      "  time_since_restore: 25.112008333206177\n",
      "  time_this_iter_s: 3.138056755065918\n",
      "  time_total_s: 25.112008333206177\n",
      "  timestamp: 1587502313\n",
      "  timesteps_since_restore: 28000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: dc90e01c\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 6.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/8 CPUs, 0/1 GPUs, 0.0/3.17 GiB heap, 0.0/1.07 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ParaKnapsack-v0_dc90e01c</td><td>RUNNING </td><td>192.168.0.11:27221</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">          25.112</td><td style=\"text-align: right;\">      28000</td><td style=\"text-align: right;\">  29.251</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ParaKnapsack-v0_dc90e01c:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-04-21_15-51-59\n",
      "  done: false\n",
      "  episode_len_mean: 8.908482142857142\n",
      "  episode_reward_max: 63.0\n",
      "  episode_reward_mean: 31.316964285714285\n",
      "  episode_reward_min: -68.0\n",
      "  episodes_this_iter: 448\n",
      "  episodes_total: 6364\n",
      "  experiment_id: b1c32acb32ee4cbb9559a4c4cbe3d7d5\n",
      "  experiment_tag: '0'\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 1644.757\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.447750985622406\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00445684464648366\n",
      "        policy_loss: -0.015289338305592537\n",
      "        total_loss: 172.96734619140625\n",
      "        vf_explained_var: 0.37308651208877563\n",
      "        vf_loss: 172.9796142578125\n",
      "    load_time_ms: 5.901\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 35712\n",
      "    sample_time_ms: 1776.504\n",
      "    update_time_ms: 51.129\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.099999999999994\n",
      "    ram_util_percent: 38.9\n",
      "  pid: 27221\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.049675876145642624\n",
      "    mean_inference_ms: 0.5269843463214389\n",
      "    mean_processing_ms: 0.20789078582240145\n",
      "  time_since_restore: 31.42128014564514\n",
      "  time_this_iter_s: 3.1705009937286377\n",
      "  time_total_s: 31.42128014564514\n",
      "  timestamp: 1587502319\n",
      "  timesteps_since_restore: 36000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: dc90e01c\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 6.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/8 CPUs, 0/1 GPUs, 0.0/3.17 GiB heap, 0.0/1.07 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ParaKnapsack-v0_dc90e01c</td><td>RUNNING </td><td>192.168.0.11:27221</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         31.4213</td><td style=\"text-align: right;\">      36000</td><td style=\"text-align: right;\">  31.317</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ParaKnapsack-v0_dc90e01c:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-04-21_15-52-02\n",
      "  done: true\n",
      "  episode_len_mean: 9.246543778801843\n",
      "  episode_reward_max: 64.0\n",
      "  episode_reward_mean: 34.085253456221196\n",
      "  episode_reward_min: -70.0\n",
      "  episodes_this_iter: 434\n",
      "  episodes_total: 6798\n",
      "  experiment_id: b1c32acb32ee4cbb9559a4c4cbe3d7d5\n",
      "  experiment_tag: '0'\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 1642.482\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.38688457012176514\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008526529185473919\n",
      "        policy_loss: -0.019615719094872475\n",
      "        total_loss: 90.95699310302734\n",
      "        vf_explained_var: 0.5279351472854614\n",
      "        vf_loss: 90.97373962402344\n",
      "    load_time_ms: 5.421\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 39680\n",
      "    sample_time_ms: 1750.981\n",
      "    update_time_ms: 46.346\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.2\n",
      "    ram_util_percent: 38.9\n",
      "  pid: 27221\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.04969941728571901\n",
      "    mean_inference_ms: 0.5263989966091744\n",
      "    mean_processing_ms: 0.20461619796892128\n",
      "  time_since_restore: 34.575931787490845\n",
      "  time_this_iter_s: 3.154651641845703\n",
      "  time_total_s: 34.575931787490845\n",
      "  timestamp: 1587502322\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: dc90e01c\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 6.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/3.17 GiB heap, 0.0/1.07 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ParaKnapsack-v0_dc90e01c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         34.5759</td><td style=\"text-align: right;\">      40000</td><td style=\"text-align: right;\"> 34.0853</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-21 15:52:02,933\tINFO tune.py:334 -- Returning an analysis object by default. You can call `analysis.trials` to retrieve a list of trials. This message will be removed in future versions of Tune.\n"
     ]
    }
   ],
   "source": [
    "results = tune.run(\n",
    "        \"PPO\",\n",
    "        stop={\"training_iteration\": 10},\n",
    "        config={\n",
    "            \"env\": \"ParaKnapsack-v0\"\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_len_mean</th>\n",
       "      <th>episodes_this_iter</th>\n",
       "      <th>timesteps_this_iter</th>\n",
       "      <th>done</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>episodes_total</th>\n",
       "      <th>training_iteration</th>\n",
       "      <th>...</th>\n",
       "      <th>info/learner/default_policy/cur_lr</th>\n",
       "      <th>info/learner/default_policy/total_loss</th>\n",
       "      <th>info/learner/default_policy/policy_loss</th>\n",
       "      <th>info/learner/default_policy/vf_loss</th>\n",
       "      <th>info/learner/default_policy/vf_explained_var</th>\n",
       "      <th>info/learner/default_policy/kl</th>\n",
       "      <th>info/learner/default_policy/entropy</th>\n",
       "      <th>info/learner/default_policy/entropy_coeff</th>\n",
       "      <th>config/env</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64.0</td>\n",
       "      <td>-70.0</td>\n",
       "      <td>34.085253</td>\n",
       "      <td>9.246544</td>\n",
       "      <td>434</td>\n",
       "      <td>4000</td>\n",
       "      <td>True</td>\n",
       "      <td>40000</td>\n",
       "      <td>6798</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>90.95699</td>\n",
       "      <td>-0.019616</td>\n",
       "      <td>90.97374</td>\n",
       "      <td>0.527935</td>\n",
       "      <td>0.008527</td>\n",
       "      <td>0.386885</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ParaKnapsack-v0</td>\n",
       "      <td>/home/christian/ray_results/PPO/PPO_ParaKnapsa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
       "0                64.0               -70.0            34.085253   \n",
       "\n",
       "   episode_len_mean  episodes_this_iter  timesteps_this_iter  done  \\\n",
       "0          9.246544                 434                 4000  True   \n",
       "\n",
       "   timesteps_total  episodes_total  training_iteration  ...  \\\n",
       "0            40000            6798                  10  ...   \n",
       "\n",
       "  info/learner/default_policy/cur_lr info/learner/default_policy/total_loss  \\\n",
       "0                            0.00005                               90.95699   \n",
       "\n",
       "   info/learner/default_policy/policy_loss  \\\n",
       "0                                -0.019616   \n",
       "\n",
       "   info/learner/default_policy/vf_loss  \\\n",
       "0                             90.97374   \n",
       "\n",
       "   info/learner/default_policy/vf_explained_var  \\\n",
       "0                                      0.527935   \n",
       "\n",
       "   info/learner/default_policy/kl info/learner/default_policy/entropy  \\\n",
       "0                        0.008527                            0.386885   \n",
       "\n",
       "  info/learner/default_policy/entropy_coeff       config/env  \\\n",
       "0                                       0.0  ParaKnapsack-v0   \n",
       "\n",
       "                                              logdir  \n",
       "0  /home/christian/ray_results/PPO/PPO_ParaKnapsa...  \n",
       "\n",
       "[1 rows x 46 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = results.dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import or_gym\n",
    "from or_gym.algos.rl_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = or_gym.make('VMPacking-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_env('VMPacking-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-21 17:18:35,675\tWARNING services.py:597 -- setpgrp failed, processes may not be cleaned up properly: [Errno 1] Operation not permitted.\n",
      "2020-04-21 17:18:35,677\tINFO resource_spec.py:216 -- Starting Ray with 2.88 GiB memory available for workers and up to 1.46 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-04-21 17:18:36,236\tINFO ray_trial_executor.py:121 -- Trial PPO_VMPacking-v0_0b54171e: Setting up new remote runner.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 5.1/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/8 CPUs, 0/1 GPUs, 0.0/2.88 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_VMPacking-v0_0b54171e</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30881)\u001b[0m 2020-04-21 17:18:39,333\tINFO trainer.py:371 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=30881)\u001b[0m 2020-04-21 17:18:39,335\tINFO trainer.py:512 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=30881)\u001b[0m 2020-04-21 17:18:41,681\tWARNING util.py:45 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-21 17:18:43,226\tERROR trial_runner.py:482 -- Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 426, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py\", line 378, in fetch_result\n",
      "    result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)\n",
      "  File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/worker.py\", line 1457, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::PPO.train()\u001b[39m (pid=30881, ip=192.168.0.11)\n",
      "  File \"python/ray/_raylet.pyx\", line 636, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 619, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\", line 444, in train\n",
      "    raise e\n",
      "  File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\", line 433, in train\n",
      "    result = Trainable.train(self)\n",
      "  File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/tune/trainable.py\", line 176, in train\n",
      "    result = self._train()\n",
      "  File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py\", line 129, in _train\n",
      "    fetches = self.optimizer.step()\n",
      "  File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/optimizers/multi_gpu_optimizer.py\", line 140, in step\n",
      "    self.num_envs_per_worker, self.train_batch_size)\n",
      "  File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/optimizers/rollout.py\", line 29, in collect_samples\n",
      "    next_sample = ray_get_and_free(fut_sample)\n",
      "  File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/utils/memory.py\", line 33, in ray_get_and_free\n",
      "    result = ray.get(object_ids)\n",
      "ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::RolloutWorker.sample()\u001b[39m (pid=30879, ip=192.168.0.11)\n",
      "  File \"python/ray/_raylet.pyx\", line 636, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 619, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 471, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py\", line 56, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py\", line 99, in get_data\n",
      "    item = next(self.rollout_provider)\n",
      "  File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py\", line 319, in _env_runner\n",
      "    soft_horizon, no_done_at_end)\n",
      "  File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py\", line 407, in _process_observations\n",
      "    policy_id).transform(raw_obs)\n",
      "  File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/models/preprocessors.py\", line 236, in transform\n",
      "    self.write(observation, array, 0)\n",
      "  File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/models/preprocessors.py\", line 244, in write\n",
      "    (len(observation), len(self.preprocessors))\n",
      "AssertionError: (4, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 6.4/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/2.88 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 1 (1 ERROR)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_VMPacking-v0_0b54171e</td><td>ERROR   </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                     </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_VMPacking-v0_0b54171e</td><td style=\"text-align: right;\">           1</td><td>/home/christian/ray_results/PPO/PPO_VMPacking-v0_0b54171e_2020-04-21_17-18-36glxpfvz9/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 6.4/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/2.88 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 1 (1 ERROR)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_VMPacking-v0_0b54171e</td><td>ERROR   </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                     </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_VMPacking-v0_0b54171e</td><td style=\"text-align: right;\">           1</td><td>/home/christian/ray_results/PPO/PPO_VMPacking-v0_0b54171e_2020-04-21_17-18-36glxpfvz9/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [PPO_VMPacking-v0_0b54171e])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f62acd9e5c84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;34m\"env_config\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"mask\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     },\n\u001b[0;32m----> 8\u001b[0;31m     reuse_actors=True)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, stop, config, resources_per_trial, num_samples, local_dir, upload_dir, trial_name_creator, loggers, sync_to_cloud, sync_to_driver, checkpoint_freq, checkpoint_at_end, sync_on_checkpoint, keep_checkpoints_num, checkpoint_score_attr, global_checkpoint_period, export_formats, max_failures, restore, search_alg, scheduler, with_server, server_port, verbose, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, return_trials, ray_auto_init, sync_function)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [PPO_VMPacking-v0_0b54171e])"
     ]
    }
   ],
   "source": [
    "results = tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\"training_iteration\": 10},\n",
    "    config={\n",
    "        \"env\": \"VMPacking-v0\",\n",
    "        \"env_config\": {\"mask\": True}\n",
    "    },\n",
    "    reuse_actors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_len_mean</th>\n",
       "      <th>episodes_this_iter</th>\n",
       "      <th>timesteps_this_iter</th>\n",
       "      <th>done</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>episodes_total</th>\n",
       "      <th>training_iteration</th>\n",
       "      <th>...</th>\n",
       "      <th>info/learner/default_policy/total_loss</th>\n",
       "      <th>info/learner/default_policy/policy_loss</th>\n",
       "      <th>info/learner/default_policy/vf_loss</th>\n",
       "      <th>info/learner/default_policy/vf_explained_var</th>\n",
       "      <th>info/learner/default_policy/kl</th>\n",
       "      <th>info/learner/default_policy/entropy</th>\n",
       "      <th>info/learner/default_policy/entropy_coeff</th>\n",
       "      <th>config/env</th>\n",
       "      <th>config/env_config</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-10003.661022</td>\n",
       "      <td>-12127.070317</td>\n",
       "      <td>-10395.543755</td>\n",
       "      <td>22.587571</td>\n",
       "      <td>177</td>\n",
       "      <td>4000</td>\n",
       "      <td>True</td>\n",
       "      <td>40000</td>\n",
       "      <td>1671</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>75354720.0</td>\n",
       "      <td>-0.077681</td>\n",
       "      <td>75354720.0</td>\n",
       "      <td>-6.537284e-08</td>\n",
       "      <td>0.016969</td>\n",
       "      <td>3.742035</td>\n",
       "      <td>0.0</td>\n",
       "      <td>VMPacking-v0</td>\n",
       "      <td>{'mask': True}</td>\n",
       "      <td>/home/christian/ray_results/PPO/PPO_VMPacking-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
       "0       -10003.661022       -12127.070317        -10395.543755   \n",
       "\n",
       "   episode_len_mean  episodes_this_iter  timesteps_this_iter  done  \\\n",
       "0         22.587571                 177                 4000  True   \n",
       "\n",
       "   timesteps_total  episodes_total  training_iteration  ...  \\\n",
       "0            40000            1671                  10  ...   \n",
       "\n",
       "  info/learner/default_policy/total_loss  \\\n",
       "0                             75354720.0   \n",
       "\n",
       "  info/learner/default_policy/policy_loss  \\\n",
       "0                               -0.077681   \n",
       "\n",
       "   info/learner/default_policy/vf_loss  \\\n",
       "0                           75354720.0   \n",
       "\n",
       "   info/learner/default_policy/vf_explained_var  \\\n",
       "0                                 -6.537284e-08   \n",
       "\n",
       "   info/learner/default_policy/kl  info/learner/default_policy/entropy  \\\n",
       "0                        0.016969                             3.742035   \n",
       "\n",
       "  info/learner/default_policy/entropy_coeff    config/env  config/env_config  \\\n",
       "0                                       0.0  VMPacking-v0     {'mask': True}   \n",
       "\n",
       "                                              logdir  \n",
       "0  /home/christian/ray_results/PPO/PPO_VMPacking-...  \n",
       "\n",
       "[1 rows x 47 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = results.dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VMPackingEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.cup_capacity = 1\n",
    "        self.mem_capacity = 1\n",
    "        self.t_interval = 20\n",
    "        self.tol = 1e-5\n",
    "#         self.step_limit = int(60 * 24 / self.t_interval)\n",
    "        self.step_limit = 15\n",
    "        self.n_pms = 50\n",
    "        self.load_idx = np.array([1, 2])\n",
    "        self.seed = 0\n",
    "        self.mask = True\n",
    "        assign_env_config(self, kwargs)\n",
    "        self.action_space = spaces.Discrete(self.n_pms)\n",
    "        if self.mask:\n",
    "            self.observation_space = spaces.Dict({\n",
    "                \"action_mask\": spaces.Box(0, 1, shape=(self.n_pms,)),\n",
    "                \"avail_actions\": spaces.Box(0, 1, shape=(self.n_pms,)),\n",
    "#                 \"avail_actions\": spaces.Discrete(self.n_pms),\n",
    "                \"data_center\": spaces.Box(0, 1, shape=(self.n_pms+1, 3))\n",
    "            })\n",
    "        else:\n",
    "            self.observation_space = spaces.Dict({\n",
    "                \"data_center\": spaces.Box(0, 1, shape=(self.n_pms+1, 3))\n",
    "            })\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.demand = self.generate_demand()\n",
    "        self.current_step = 0\n",
    "        if self.mask:\n",
    "            self.state = {\n",
    "                \"action_mask\": np.ones(self.n_pms),\n",
    "                \"avail_actions\": np.ones(self.n_pms),\n",
    "                \"data_center\": np.vstack([\n",
    "                    np.zeros((self.n_pms, 3)),\n",
    "                    self.demand[self.current_step]])\n",
    "            }\n",
    "        else:\n",
    "            self.state = {\n",
    "                \"data_center\": np.vstack([\n",
    "                    np.zeros((self.n_pms, 3)),\n",
    "                    self.demand[self.current_step]])\n",
    "            }\n",
    "        self.assignment = {}\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        pm_state = self.state[\"data_center\"][:-1]\n",
    "        demand = self.state[\"data_center\"][-1, 1:]\n",
    "        \n",
    "        if action < 0 or action >= self.n_pms:\n",
    "            raise ValueError(\"Invalid action: {}\".format(action))\n",
    "            \n",
    "        elif any(pm_state[action, 1:] + demand > 1 + self.tol):\n",
    "            # Demand doesn't fit into PM\n",
    "            reward = -10000\n",
    "            done = True\n",
    "        else:\n",
    "            if pm_state[action, 0] == 0:\n",
    "                # Open PM if closed\n",
    "                pm_state[action, 0] = 1\n",
    "            pm_state[action, self.load_idx] += demand\n",
    "            reward = np.sum(pm_state[:, 0] * (pm_state[:,1:].sum(axis=1) - 2))\n",
    "            self.assignment[self.current_step] = action\n",
    "            \n",
    "        self.current_step += 1\n",
    "        if self.current_step >= self.step_limit:\n",
    "            done = True\n",
    "        self.update_state(pm_state)\n",
    "        return self.state, reward, done, {}\n",
    "    \n",
    "    def update_state(self, pm_state):\n",
    "        # Make action selection impossible if the PM would exceed capacity\n",
    "        step = self.current_step if self.current_step < self.step_limit else self.step_limit-1\n",
    "        data_center = np.vstack([pm_state, self.demand[step]])\n",
    "        data_center = np.where(data_center>1,1,data_center) # Fix rounding errors\n",
    "        self.state[\"data_center\"] = data_center\n",
    "        if self.mask:\n",
    "            action_mask = (pm_state[:, 1:] + self.demand[step, 1:]) <= 1\n",
    "            self.state[\"action_mask\"] = (action_mask.sum(axis=1)==2).astype(int)\n",
    "                    \n",
    "    def generate_demand(self):\n",
    "        cpu_demand = np.random.uniform(0, 1, size=self.step_limit)\n",
    "        mem_demand = np.random.uniform(0, 1, size=self.step_limit)\n",
    "        return np.vstack([np.zeros(self.step_limit), cpu_demand, mem_demand]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env(config_env):\n",
    "    return VMPackingEnv()\n",
    "\n",
    "tune.register_env(\"VMPacking-v2\", lambda config: create_env(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-22 09:07:07,426\tINFO ray_trial_executor.py:121 -- Trial PPO_VMPacking-v2_8d0c9586: Setting up new remote runner.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 5.8/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/8 CPUs, 0/1 GPUs, 0.0/2.39 GiB heap, 0.0/0.83 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_VMPacking-v2_8d0c9586</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=23676)\u001b[0m 2020-04-22 09:07:09,427\tINFO trainer.py:371 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=23676)\u001b[0m 2020-04-22 09:07:09,429\tINFO trainer.py:512 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=23676)\u001b[0m 2020-04-22 09:07:11,922\tWARNING util.py:45 -- Install gputil for GPU system monitoring.\n",
      "Result for PPO_VMPacking-v2_8d0c9586:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-04-22_09-07-18\n",
      "  done: true\n",
      "  episode_len_mean: 9.825552825552826\n",
      "  episode_reward_max: -73.1885195576232\n",
      "  episode_reward_mean: -8601.61104317446\n",
      "  episode_reward_min: -10121.984126281812\n",
      "  episodes_this_iter: 407\n",
      "  episodes_total: 407\n",
      "  experiment_id: a5ce0042a5184bbfae50ea342a45191a\n",
      "  experiment_tag: '0'\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 2551.644\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.8979239463806152\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014262373559176922\n",
      "        policy_loss: -0.054376937448978424\n",
      "        total_loss: 70013248.0\n",
      "        vf_explained_var: -3.268641890485924e-08\n",
      "        vf_loss: 70013248.0\n",
      "    load_time_ms: 52.427\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 3968\n",
      "    sample_time_ms: 3367.581\n",
      "    update_time_ms: 453.403\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.07000000000001\n",
      "    ram_util_percent: 43.510000000000005\n",
      "  pid: 23676\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.12600473678557003\n",
      "    mean_inference_ms: 0.5507379782757449\n",
      "    mean_processing_ms: 0.16830557793398995\n",
      "  time_since_restore: 6.465942621231079\n",
      "  time_this_iter_s: 6.465942621231079\n",
      "  time_total_s: 6.465942621231079\n",
      "  timestamp: 1587564438\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 8d0c9586\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 6.8/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/2.39 GiB heap, 0.0/0.83 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_VMPacking-v2_8d0c9586</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.46594</td><td style=\"text-align: right;\">       4000</td><td style=\"text-align: right;\">-8601.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=23676)\u001b[0m 2020-04-22 09:07:18,462\tWARNING ppo.py:144 -- The magnitude of your environment rewards are more than 860.0x the scale of `vf_clip_param`. This means that it will take more than 860.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 6.8/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/2.39 GiB heap, 0.0/0.83 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_VMPacking-v2_8d0c9586</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.46594</td><td style=\"text-align: right;\">       4000</td><td style=\"text-align: right;\">-8601.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-22 09:07:18,486\tINFO tune.py:334 -- Returning an analysis object by default. You can call `analysis.trials` to retrieve a list of trials. This message will be removed in future versions of Tune.\n"
     ]
    }
   ],
   "source": [
    "results = tune.run(\n",
    "        \"PPO\",\n",
    "        stop={\"training_iteration\": 1},\n",
    "        config={\n",
    "            \"env\": \"VMPacking-v2\"\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_len_mean</th>\n",
       "      <th>episodes_this_iter</th>\n",
       "      <th>timesteps_this_iter</th>\n",
       "      <th>done</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>episodes_total</th>\n",
       "      <th>training_iteration</th>\n",
       "      <th>...</th>\n",
       "      <th>info/learner/default_policy/cur_lr</th>\n",
       "      <th>info/learner/default_policy/total_loss</th>\n",
       "      <th>info/learner/default_policy/policy_loss</th>\n",
       "      <th>info/learner/default_policy/vf_loss</th>\n",
       "      <th>info/learner/default_policy/vf_explained_var</th>\n",
       "      <th>info/learner/default_policy/kl</th>\n",
       "      <th>info/learner/default_policy/entropy</th>\n",
       "      <th>info/learner/default_policy/entropy_coeff</th>\n",
       "      <th>config/env</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-73.18852</td>\n",
       "      <td>-10121.984126</td>\n",
       "      <td>-8601.611043</td>\n",
       "      <td>9.825553</td>\n",
       "      <td>407</td>\n",
       "      <td>4000</td>\n",
       "      <td>True</td>\n",
       "      <td>4000</td>\n",
       "      <td>407</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>70013250.0</td>\n",
       "      <td>-0.054377</td>\n",
       "      <td>70013250.0</td>\n",
       "      <td>-3.268642e-08</td>\n",
       "      <td>0.014262</td>\n",
       "      <td>3.897924</td>\n",
       "      <td>0.0</td>\n",
       "      <td>VMPacking-v2</td>\n",
       "      <td>/home/christian/ray_results/PPO/PPO_VMPacking-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
       "0           -73.18852       -10121.984126         -8601.611043   \n",
       "\n",
       "   episode_len_mean  episodes_this_iter  timesteps_this_iter  done  \\\n",
       "0          9.825553                 407                 4000  True   \n",
       "\n",
       "   timesteps_total  episodes_total  training_iteration  ...  \\\n",
       "0             4000             407                   1  ...   \n",
       "\n",
       "  info/learner/default_policy/cur_lr info/learner/default_policy/total_loss  \\\n",
       "0                            0.00005                             70013250.0   \n",
       "\n",
       "   info/learner/default_policy/policy_loss  \\\n",
       "0                                -0.054377   \n",
       "\n",
       "   info/learner/default_policy/vf_loss  \\\n",
       "0                           70013250.0   \n",
       "\n",
       "   info/learner/default_policy/vf_explained_var  \\\n",
       "0                                 -3.268642e-08   \n",
       "\n",
       "   info/learner/default_policy/kl info/learner/default_policy/entropy  \\\n",
       "0                        0.014262                            3.897924   \n",
       "\n",
       "  info/learner/default_policy/entropy_coeff    config/env  \\\n",
       "0                                       0.0  VMPacking-v2   \n",
       "\n",
       "                                              logdir  \n",
       "0  /home/christian/ray_results/PPO/PPO_VMPacking-...  \n",
       "\n",
       "[1 rows x 46 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = results.dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from or_gym.algos.vm_packing.heuristics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VMPackingEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()['avail_actions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_fit_heuristic(env):\n",
    "    # assert env.spec.id == ('VMPacking-v0' or 'VMPacking-v1'), \\\n",
    "        # '{} received. Heuristic designed for VMPacking-v0/v1.'.format(env.spec.id)\n",
    "\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards, actions = [], []\n",
    "    while done == False:\n",
    "        action = first_fit_step(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "    return actions, rewards\n",
    "\n",
    "def first_fit_step(state):\n",
    "    s_bins = state['data_center'][:-1]\n",
    "    s_item = state['data_center'][-1, 1:]\n",
    "    action = None\n",
    "    open_bins = np.where(s_bins[:,0]==1)[0]\n",
    "    if len(open_bins) < 1:\n",
    "        # Open first bin for item\n",
    "        action = 0\n",
    "    else:\n",
    "        # Check each bin until one is found to fit the item\n",
    "        for b in open_bins:\n",
    "            if all(s_bins[b, [1, 2]] + s_item <= 1):\n",
    "                action = b\n",
    "        if action is None:\n",
    "            action = np.max(open_bins) + 1\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    env.reset()\n",
    "    actions, rewards = first_fit_heuristic(env)\n",
    "    tot_rewards = sum(rewards)\n",
    "    if tot_rewards > 0:\n",
    "        print('Positive rewards found')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VMPackingEnv()\n",
    "N = 1000\n",
    "count = 0\n",
    "while count < N:\n",
    "    env.reset()\n",
    "    p_found = False\n",
    "    done = False\n",
    "    rewards = []\n",
    "    while done == False:\n",
    "        action = env.action_space.sample()\n",
    "        state, r, done, _ = env.step(action)\n",
    "        rewards.append(r)\n",
    "        if r > 0:\n",
    "            print('Positive rewards found')\n",
    "            p_found = True\n",
    "            break\n",
    "    count += 1\n",
    "    if p_found:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.245679701610709"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_state = state['data_center'][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pm_state[:, 0] * (pm_state[:,1:].sum(axis=1) - 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 6, 1: 6}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action_mask': array([1, 1, 1, 1, 1, 1, 0, 1, 1, 1]),\n",
       " 'avail_actions': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'data_center': array([[0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.98100702, 0.64886836]])}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.8077121 , 0.78294696])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.demand[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.76834515, 0.8866755 ])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.demand[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand = state[\"data_center\"][-1, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98100702, 0.64886836])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_state = env.state[\"data_center\"][:-1]\n",
    "demand = env.state[\"data_center\"][-1, 1:]\n",
    "\n",
    "if action < 0 or action >= env.n_pms:\n",
    "    raise ValueError(\"Invalid action: {}\".format(action))\n",
    "\n",
    "elif any(pm_state[action, 1:] + demand > 1):\n",
    "    # Demand doesn't fit into PM\n",
    "    reward = -10000\n",
    "    done = True\n",
    "else:\n",
    "    print('Pack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-10000"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
