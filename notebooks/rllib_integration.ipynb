{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.models.tf.fcnet_v2 import FullyConnectedNetwork\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.utils import try_import_tf\n",
    "from ray.tune import grid_search, register_env\n",
    "import or_gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from or_gym.envs.classic_or.knapsack import KnapsackEnv\n",
    "# from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnapsackEnv1(gym.Env):\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        try:\n",
    "            self.item_weights = weights\n",
    "            self.item_values = values\n",
    "        except NameError:\n",
    "            self.item_values = np.random.randint(30, size=200)\n",
    "            self.item_weights = np.random.randint(1, 20, size=200)\n",
    "        self.item_numbers = np.arange(len(self.item_weights))\n",
    "        self.N = len(self.item_weights)\n",
    "        self.max_weight = 200\n",
    "        self.current_weight = 0\n",
    "        \n",
    "        self.action_space = spaces.Discrete(self.N)\n",
    "        self.observation_space = spaces.Box(\n",
    "            0, self.max_weight, shape=(2, self.N + 1), dtype=np.int16)\n",
    "#         self.observation_space = spaces.Tuple((\n",
    "#             spaces.Discrete(self.N),\n",
    "#             spaces.Discrete(self.N),\n",
    "#             spaces.Box(0, self.max_weight, shape=(2,), dtype=np.int32)))\n",
    "#         self.observation_space = spaces.Tuple((\n",
    "#             spaces.Box(0, self.max_weight, shape=(self.N,), dtype=np.int32),\n",
    "#             spaces.Box(0, self.max_weight, shape=(self.N,), dtype=np.int32),\n",
    "#             spaces.Box(0, self.max_weight, shape=(2,), dtype=np.int32)\n",
    "#         ))\n",
    "#         self.observation_space = spaces.Box(\n",
    "#             0, self.max_weight, shape=(2,), dtype=np.int32)\n",
    "        \n",
    "        self.seed()\n",
    "        self.reset()\n",
    "        \n",
    "    def step(self, item):\n",
    "        # Check that item will fit\n",
    "        if self.item_weights[item] + self.current_weight <= self.max_weight:\n",
    "            self.current_weight += self.item_weights[item]\n",
    "            reward = self.item_values[item]\n",
    "            if self.current_weight == self.max_weight:\n",
    "                done = True\n",
    "            else:\n",
    "                done = False\n",
    "        else:\n",
    "            # End trial if over weight\n",
    "            reward = 0\n",
    "            done = True\n",
    "            \n",
    "        self._update_state()\n",
    "        return self.state, reward, done, {}\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        return self.state\n",
    "    \n",
    "    def _update_state(self):\n",
    "        self.state = np.vstack([\n",
    "            self.item_weights,\n",
    "            self.item_values\n",
    "        ])\n",
    "        self.state = np.hstack([\n",
    "            self.state, \n",
    "            np.array([[self.max_weight],\n",
    "                      [self.current_weight]])\n",
    "        ])\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_weight = 0\n",
    "        self._update_state()\n",
    "        return self.state\n",
    "    \n",
    "    def sample_action(self):\n",
    "        return np.random.choice(self.item_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Knapsack-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 201)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tuple(Discrete(2), Discrete(2))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spaces.Tuple((spaces.Box(0, 200, shape=(1,)),\n",
    "              spaces.Box(0, 200, shape=(1,))))\n",
    "spaces.Tuple((spaces.Discrete(2), \n",
    "              spaces.Discrete(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 11), (2, 1))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = np.arange(10), np.arange(10)\n",
    "c, d = 11, 12\n",
    "x = np.vstack([a, b])\n",
    "x = np.hstack([x, np.array([[c], [d]])])\n",
    "y = np.array([[c], [d]])\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 18:53:55,173\tWARNING services.py:597 -- setpgrp failed, processes may not be cleaned up properly: [Errno 1] Operation not permitted.\n",
      "2020-03-17 18:53:55,174\tINFO resource_spec.py:216 -- Starting Ray with 4.2 GiB memory available for workers and up to 2.11 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-03-17 18:53:55,748\tINFO ray_trial_executor.py:121 -- Trial PPO_Knapsack-v0_8ff22748: Setting up new remote runner.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 6.4/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 0/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 3 (1 RUNNING, 2 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc  </th><th>lr  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Knapsack-v0_8ff22748</td><td>RUNNING </td><td>     </td><td>    </td></tr>\n",
       "<tr><td>PPO_Knapsack-v0_8ff5199e</td><td>PENDING </td><td>     </td><td>    </td></tr>\n",
       "<tr><td>PPO_Knapsack-v0_8ff71dde</td><td>PENDING </td><td>     </td><td>    </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 18:53:55,916\tINFO ray_trial_executor.py:121 -- Trial PPO_Knapsack-v0_8ff5199e: Setting up new remote runner.\n",
      "2020-03-17 18:53:55,961\tINFO ray_trial_executor.py:121 -- Trial PPO_Knapsack-v0_8ff71dde: Setting up new remote runner.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=31969)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=31969)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=31969)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=31969)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=31969)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=31969)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=31969)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=31969)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=31969)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=31969)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=31969)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=31969)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=31968)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=31968)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=31968)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=31968)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=31968)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=31968)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=31968)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=31968)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=31968)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=31968)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=31968)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=31968)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=31972)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=31972)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=31972)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=31972)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=31972)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=31972)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=31972)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=31972)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=31972)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=31972)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=31972)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=31972)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=31968)\u001b[0m 2020-03-17 18:53:58,734\tINFO trainer.py:371 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=31968)\u001b[0m 2020-03-17 18:53:58,736\tINFO trainer.py:512 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=31969)\u001b[0m 2020-03-17 18:53:58,781\tINFO trainer.py:371 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=31969)\u001b[0m 2020-03-17 18:53:58,783\tINFO trainer.py:512 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=31972)\u001b[0m 2020-03-17 18:53:58,797\tINFO trainer.py:371 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=31972)\u001b[0m 2020-03-17 18:53:58,800\tINFO trainer.py:512 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=31968)\u001b[0m WARNING:tensorflow:From /home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:68: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=31968)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=31968)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=31968)\u001b[0m WARNING:tensorflow:From /home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:73: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=31968)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=31968)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=31969)\u001b[0m WARNING:tensorflow:From /home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:68: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=31969)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=31969)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=31969)\u001b[0m WARNING:tensorflow:From /home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:73: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=31969)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=31969)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=31972)\u001b[0m WARNING:tensorflow:From /home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:68: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=31972)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=31972)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=31972)\u001b[0m WARNING:tensorflow:From /home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:73: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=31972)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=31972)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=31968)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=31968)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=31972)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=31972)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=31969)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=31969)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32110)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32110)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32110)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32110)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32110)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32110)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32110)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32110)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32110)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32110)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32110)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32110)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=31974)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=31974)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=31974)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=31974)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=31974)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=31974)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=31974)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=31974)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=31974)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=31974)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=31974)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=31974)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=31968)\u001b[0m 2020-03-17 18:54:03,107\tWARNING util.py:45 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=31969)\u001b[0m 2020-03-17 18:54:03,254\tWARNING util.py:45 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=31972)\u001b[0m 2020-03-17 18:54:03,276\tWARNING util.py:45 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m WARNING:tensorflow:From /home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:68: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m WARNING:tensorflow:From /home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:73: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=32110)\u001b[0m WARNING:tensorflow:From /home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:68: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=32110)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=32110)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=32110)\u001b[0m WARNING:tensorflow:From /home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:73: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=32110)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=32110)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=31974)\u001b[0m WARNING:tensorflow:From /home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:68: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=31974)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=31974)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=31974)\u001b[0m WARNING:tensorflow:From /home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:73: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=31974)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=31974)\u001b[0m keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=31974)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=31974)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=32110)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=32110)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "Result for PPO_Knapsack-v0_8ff71dde:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-17_18-54-16\n",
      "  done: false\n",
      "  episode_len_mean: 21.116402116402117\n",
      "  episode_reward_max: 386.0\n",
      "  episode_reward_mean: 260.70899470899474\n",
      "  episode_reward_min: 135.0\n",
      "  episodes_this_iter: 189\n",
      "  episodes_total: 189\n",
      "  experiment_id: d3f0529a6d9245179c750a145ebbd525\n",
      "  experiment_tag: 2_lr=1e-06\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 8852.399\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 9.999999974752427e-07\n",
      "        entropy: 5.297819137573242\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0004841416666749865\n",
      "        policy_loss: -0.007955948822200298\n",
      "        total_loss: 20465.3046875\n",
      "        vf_explained_var: 0.00018114043632522225\n",
      "        vf_loss: 20465.310546875\n",
      "    load_time_ms: 84.975\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 3968\n",
      "    sample_time_ms: 4372.439\n",
      "    update_time_ms: 367.105\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.70499999999998\n",
      "    ram_util_percent: 55.485\n",
      "  pid: 31968\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07863755048557805\n",
      "    mean_inference_ms: 0.678935130814617\n",
      "    mean_processing_ms: 0.15518820842961253\n",
      "  time_since_restore: 13.714516401290894\n",
      "  time_this_iter_s: 13.714516401290894\n",
      "  time_total_s: 13.714516401290894\n",
      "  timestamp: 1584489256\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 8ff71dde\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6/8 CPUs, 0/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 3 (3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Knapsack-v0_8ff22748</td><td>RUNNING </td><td>                  </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>PPO_Knapsack-v0_8ff5199e</td><td>RUNNING </td><td>                  </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>PPO_Knapsack-v0_8ff71dde</td><td>RUNNING </td><td>192.168.0.11:31968</td><td style=\"text-align: right;\">1e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         13.7145</td><td style=\"text-align: right;\">       4000</td><td style=\"text-align: right;\"> 260.709</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Knapsack-v0_8ff22748:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-17_18-54-16\n",
      "  done: false\n",
      "  episode_len_mean: 20.49230769230769\n",
      "  episode_reward_max: 381.0\n",
      "  episode_reward_mean: 252.48717948717947\n",
      "  episode_reward_min: 131.0\n",
      "  episodes_this_iter: 195\n",
      "  episodes_total: 195\n",
      "  experiment_id: 23b8c42222264ac39550581747d91c02\n",
      "  experiment_tag: 0_lr=0.01\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 8808.329\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 1.7222822904586792\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 14.326783180236816\n",
      "        policy_loss: 0.6111807823181152\n",
      "        total_loss: 16856.98828125\n",
      "        vf_explained_var: 3.845460838647341e-09\n",
      "        vf_loss: 16853.51171875\n",
      "    load_time_ms: 85.413\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 3968\n",
      "    sample_time_ms: 4292.357\n",
      "    update_time_ms: 345.843\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.76\n",
      "    ram_util_percent: 55.545\n",
      "  pid: 31972\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07855311181359936\n",
      "    mean_inference_ms: 0.6803179704913556\n",
      "    mean_processing_ms: 0.15602347791805712\n",
      "  time_since_restore: 13.56846833229065\n",
      "  time_this_iter_s: 13.56846833229065\n",
      "  time_total_s: 13.56846833229065\n",
      "  timestamp: 1584489256\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 8ff22748\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=31968)\u001b[0m 2020-03-17 18:54:16,905\tWARNING ppo.py:129 -- The magnitude of your value function loss is extremely large (20465.310546875) compared to the policy loss (-0.007955948822200298). This can prevent the policy from learning. Consider scaling down the VF loss by reducing vf_loss_coeff, or disabling vf_share_layers.\n",
      "\u001b[2m\u001b[36m(pid=31972)\u001b[0m 2020-03-17 18:54:16,928\tWARNING ppo.py:129 -- The magnitude of your value function loss is extremely large (16853.51171875) compared to the policy loss (0.6111807823181152). This can prevent the policy from learning. Consider scaling down the VF loss by reducing vf_loss_coeff, or disabling vf_share_layers.\n",
      "Result for PPO_Knapsack-v0_8ff5199e:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-17_18-54-17\n",
      "  done: false\n",
      "  episode_len_mean: 20.78125\n",
      "  episode_reward_max: 390.0\n",
      "  episode_reward_mean: 254.10416666666666\n",
      "  episode_reward_min: 159.0\n",
      "  episodes_this_iter: 192\n",
      "  episodes_total: 192\n",
      "  experiment_id: 0aded5c696294f66ba676c50eb86e48a\n",
      "  experiment_tag: 1_lr=0.0001\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 8887.438\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 9.999999747378752e-05\n",
      "        entropy: 5.261809825897217\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03728960081934929\n",
      "        policy_loss: -0.056404661387205124\n",
      "        total_loss: 17400.384765625\n",
      "        vf_explained_var: 3.0058045012992807e-05\n",
      "        vf_loss: 17400.435546875\n",
      "    load_time_ms: 72.928\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 3968\n",
      "    sample_time_ms: 4370.821\n",
      "    update_time_ms: 364.165\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.745\n",
      "    ram_util_percent: 55.535000000000004\n",
      "  pid: 31969\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07792592972286104\n",
      "    mean_inference_ms: 0.6786770476188934\n",
      "    mean_processing_ms: 0.15563828979126068\n",
      "  time_since_restore: 13.72456407546997\n",
      "  time_this_iter_s: 13.72456407546997\n",
      "  time_total_s: 13.72456407546997\n",
      "  timestamp: 1584489257\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 8ff5199e\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=31969)\u001b[0m 2020-03-17 18:54:17,044\tWARNING ppo.py:129 -- The magnitude of your value function loss is extremely large (17400.435546875) compared to the policy loss (-0.056404661387205124). This can prevent the policy from learning. Consider scaling down the VF loss by reducing vf_loss_coeff, or disabling vf_share_layers.\n",
      "Result for PPO_Knapsack-v0_8ff22748:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-17_18-54-29\n",
      "  done: false\n",
      "  episode_len_mean: 24.801242236024844\n",
      "  episode_reward_max: 437.0\n",
      "  episode_reward_mean: 289.42857142857144\n",
      "  episode_reward_min: 154.0\n",
      "  episodes_this_iter: 161\n",
      "  episodes_total: 356\n",
      "  experiment_id: 23b8c42222264ac39550581747d91c02\n",
      "  experiment_tag: 0_lr=0.01\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 8684.64\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 2.241379499435425\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.4016111493110657\n",
      "        policy_loss: 0.07505632191896439\n",
      "        total_loss: 16954.359375\n",
      "        vf_explained_var: -1.5381843354589364e-08\n",
      "        vf_loss: 16954.1640625\n",
      "    load_time_ms: 52.959\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 7936\n",
      "    sample_time_ms: 3997.352\n",
      "    update_time_ms: 175.345\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.72222222222223\n",
      "    ram_util_percent: 56.33888888888888\n",
      "  pid: 31972\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07841238721044878\n",
      "    mean_inference_ms: 0.6785989925125631\n",
      "    mean_processing_ms: 0.1535430548712844\n",
      "  time_since_restore: 25.86914896965027\n",
      "  time_this_iter_s: 12.30068063735962\n",
      "  time_total_s: 25.86914896965027\n",
      "  timestamp: 1584489269\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 8ff22748\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6/8 CPUs, 0/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 3 (3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Knapsack-v0_8ff22748</td><td>RUNNING </td><td>192.168.0.11:31972</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         25.8691</td><td style=\"text-align: right;\">       8000</td><td style=\"text-align: right;\"> 289.429</td></tr>\n",
       "<tr><td>PPO_Knapsack-v0_8ff5199e</td><td>RUNNING </td><td>192.168.0.11:31969</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         13.7246</td><td style=\"text-align: right;\">       4000</td><td style=\"text-align: right;\"> 254.104</td></tr>\n",
       "<tr><td>PPO_Knapsack-v0_8ff71dde</td><td>RUNNING </td><td>192.168.0.11:31968</td><td style=\"text-align: right;\">1e-06 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         13.7145</td><td style=\"text-align: right;\">       4000</td><td style=\"text-align: right;\"> 260.709</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=31972)\u001b[0m 2020-03-17 18:54:29,251\tWARNING ppo.py:129 -- The magnitude of your value function loss is extremely large (16954.1640625) compared to the policy loss (0.07505632191896439). This can prevent the policy from learning. Consider scaling down the VF loss by reducing vf_loss_coeff, or disabling vf_share_layers.\n",
      "Result for PPO_Knapsack-v0_8ff71dde:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-17_18-54-29\n",
      "  done: false\n",
      "  episode_len_mean: 20.880208333333332\n",
      "  episode_reward_max: 509.0\n",
      "  episode_reward_mean: 259.765625\n",
      "  episode_reward_min: 128.0\n",
      "  episodes_this_iter: 192\n",
      "  episodes_total: 381\n",
      "  experiment_id: d3f0529a6d9245179c750a145ebbd525\n",
      "  experiment_tag: 2_lr=1e-06\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 8758.365\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 9.999999974752427e-07\n",
      "        entropy: 5.296303749084473\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0009387807222083211\n",
      "        policy_loss: -0.009477922692894936\n",
      "        total_loss: 20355.326171875\n",
      "        vf_explained_var: 0.00014808293781243265\n",
      "        vf_loss: 20355.337890625\n",
      "    load_time_ms: 53.949\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 7936\n",
      "    sample_time_ms: 4054.715\n",
      "    update_time_ms: 186.239\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.69999999999999\n",
      "    ram_util_percent: 56.32222222222222\n",
      "  pid: 31968\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07867857808247905\n",
      "    mean_inference_ms: 0.6788192041485538\n",
      "    mean_processing_ms: 0.15514753055846658\n",
      "  time_since_restore: 26.156577110290527\n",
      "  time_this_iter_s: 12.442060708999634\n",
      "  time_total_s: 26.156577110290527\n",
      "  timestamp: 1584489269\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 8ff71dde\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=31968)\u001b[0m 2020-03-17 18:54:29,361\tWARNING ppo.py:129 -- The magnitude of your value function loss is extremely large (20355.337890625) compared to the policy loss (-0.009477922692894936). This can prevent the policy from learning. Consider scaling down the VF loss by reducing vf_loss_coeff, or disabling vf_share_layers.\n",
      "Result for PPO_Knapsack-v0_8ff5199e:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-17_18-54-29\n",
      "  done: false\n",
      "  episode_len_mean: 21.363636363636363\n",
      "  episode_reward_max: 452.0\n",
      "  episode_reward_mean: 280.2513368983957\n",
      "  episode_reward_min: 154.0\n",
      "  episodes_this_iter: 187\n",
      "  episodes_total: 379\n",
      "  experiment_id: 0aded5c696294f66ba676c50eb86e48a\n",
      "  experiment_tag: 1_lr=0.0001\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 8741.592\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 9.999999747378752e-05\n",
      "        entropy: 5.216785430908203\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0366441048681736\n",
      "        policy_loss: -0.05768592283129692\n",
      "        total_loss: 19351.189453125\n",
      "        vf_explained_var: 1.167481968877837e-05\n",
      "        vf_loss: 19351.234375\n",
      "    load_time_ms: 48.424\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 7936\n",
      "    sample_time_ms: 4068.995\n",
      "    update_time_ms: 183.414\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.72222222222223\n",
      "    ram_util_percent: 56.33888888888888\n",
      "  pid: 31969\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07868531256552833\n",
      "    mean_inference_ms: 0.6825054098614871\n",
      "    mean_processing_ms: 0.15582780274223226\n",
      "  time_since_restore: 26.121914863586426\n",
      "  time_this_iter_s: 12.397350788116455\n",
      "  time_total_s: 26.121914863586426\n",
      "  timestamp: 1584489269\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 8ff5199e\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=31969)\u001b[0m 2020-03-17 18:54:29,452\tWARNING ppo.py:129 -- The magnitude of your value function loss is extremely large (19351.234375) compared to the policy loss (-0.05768592283129692). This can prevent the policy from learning. Consider scaling down the VF loss by reducing vf_loss_coeff, or disabling vf_share_layers.\n",
      "Result for PPO_Knapsack-v0_8ff22748:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-17_18-54-43\n",
      "  done: true\n",
      "  episode_len_mean: 26.549668874172184\n",
      "  episode_reward_max: 570.0\n",
      "  episode_reward_mean: 346.35099337748346\n",
      "  episode_reward_min: 138.0\n",
      "  episodes_this_iter: 151\n",
      "  episodes_total: 507\n",
      "  experiment_id: 23b8c42222264ac39550581747d91c02\n",
      "  experiment_tag: 0_lr=0.01\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 9132.535\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 1.8591874837875366\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.14950326085090637\n",
      "        policy_loss: 0.02695458009839058\n",
      "        total_loss: 23719.58203125\n",
      "        vf_explained_var: 5.7681912579710115e-09\n",
      "        vf_loss: 23719.484375\n",
      "    load_time_ms: 40.389\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 11904\n",
      "    sample_time_ms: 3947.484\n",
      "    update_time_ms: 118.068\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.06315789473685\n",
      "    ram_util_percent: 56.3421052631579\n",
      "  pid: 31972\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.0788812070734828\n",
      "    mean_inference_ms: 0.682501999122601\n",
      "    mean_processing_ms: 0.1527470018990625\n",
      "  time_since_restore: 39.776124000549316\n",
      "  time_this_iter_s: 13.906975030899048\n",
      "  time_total_s: 39.776124000549316\n",
      "  timestamp: 1584489283\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 8ff22748\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/8 CPUs, 0/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 3 (1 TERMINATED, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Knapsack-v0_8ff22748</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         39.7761</td><td style=\"text-align: right;\">      12000</td><td style=\"text-align: right;\"> 346.351</td></tr>\n",
       "<tr><td>PPO_Knapsack-v0_8ff5199e</td><td>RUNNING   </td><td>192.168.0.11:31969</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         26.1219</td><td style=\"text-align: right;\">       8000</td><td style=\"text-align: right;\"> 280.251</td></tr>\n",
       "<tr><td>PPO_Knapsack-v0_8ff71dde</td><td>RUNNING   </td><td>192.168.0.11:31968</td><td style=\"text-align: right;\">1e-06 </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         26.1566</td><td style=\"text-align: right;\">       8000</td><td style=\"text-align: right;\"> 259.766</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=31972)\u001b[0m 2020-03-17 18:54:43,175\tWARNING ppo.py:129 -- The magnitude of your value function loss is extremely large (23719.484375) compared to the policy loss (0.02695458009839058). This can prevent the policy from learning. Consider scaling down the VF loss by reducing vf_loss_coeff, or disabling vf_share_layers.\n",
      "Result for PPO_Knapsack-v0_8ff71dde:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-17_18-54-43\n",
      "  done: true\n",
      "  episode_len_mean: 21.03157894736842\n",
      "  episode_reward_max: 424.0\n",
      "  episode_reward_mean: 263.0157894736842\n",
      "  episode_reward_min: 144.0\n",
      "  episodes_this_iter: 190\n",
      "  episodes_total: 571\n",
      "  experiment_id: d3f0529a6d9245179c750a145ebbd525\n",
      "  experiment_tag: 2_lr=1e-06\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 9255.684\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.05000000074505806\n",
      "        cur_lr: 9.999999974752427e-07\n",
      "        entropy: 5.293855667114258\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.001201825449243188\n",
      "        policy_loss: -0.013342607766389847\n",
      "        total_loss: 21042.234375\n",
      "        vf_explained_var: 7.19793388270773e-05\n",
      "        vf_loss: 21042.248046875\n",
      "    load_time_ms: 42.466\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 11904\n",
      "    sample_time_ms: 3982.143\n",
      "    update_time_ms: 125.677\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.82000000000001\n",
      "    ram_util_percent: 56.330000000000005\n",
      "  pid: 31968\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07889928564251726\n",
      "    mean_inference_ms: 0.6793348920930377\n",
      "    mean_processing_ms: 0.15510186464843545\n",
      "  time_since_restore: 40.27841067314148\n",
      "  time_this_iter_s: 14.121833562850952\n",
      "  time_total_s: 40.27841067314148\n",
      "  timestamp: 1584489283\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 8ff71dde\n",
      "  \n",
      "Result for PPO_Knapsack-v0_8ff5199e:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-17_18-54-43\n",
      "  done: true\n",
      "  episode_len_mean: 22.55056179775281\n",
      "  episode_reward_max: 452.0\n",
      "  episode_reward_mean: 309.0280898876405\n",
      "  episode_reward_min: 174.0\n",
      "  episodes_this_iter: 178\n",
      "  episodes_total: 557\n",
      "  experiment_id: 0aded5c696294f66ba676c50eb86e48a\n",
      "  experiment_tag: 1_lr=0.0001\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 9202.447\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 9.999999747378752e-05\n",
      "        entropy: 5.167665481567383\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.028351083397865295\n",
      "        policy_loss: -0.04376783221960068\n",
      "        total_loss: 20552.619140625\n",
      "        vf_explained_var: 1.4182060112943873e-05\n",
      "        vf_loss: 20552.646484375\n",
      "    load_time_ms: 40.28\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 11904\n",
      "    sample_time_ms: 4004.242\n",
      "    update_time_ms: 123.291\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.595\n",
      "    ram_util_percent: 56.335\n",
      "  pid: 31969\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07908849475403824\n",
      "    mean_inference_ms: 0.6846799104275896\n",
      "    mean_processing_ms: 0.15642620922972608\n",
      "  time_since_restore: 40.1577889919281\n",
      "  time_this_iter_s: 14.035874128341675\n",
      "  time_total_s: 40.1577889919281\n",
      "  timestamp: 1584489283\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 8ff5199e\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=31969)\u001b[0m 2020-03-17 18:54:43,497\tWARNING ppo.py:129 -- The magnitude of your value function loss is extremely large (20552.646484375) compared to the policy loss (-0.04376783221960068). This can prevent the policy from learning. Consider scaling down the VF loss by reducing vf_loss_coeff, or disabling vf_share_layers.\n",
      "\u001b[2m\u001b[36m(pid=31968)\u001b[0m 2020-03-17 18:54:43,497\tWARNING ppo.py:129 -- The magnitude of your value function loss is extremely large (21042.248046875) compared to the policy loss (-0.013342607766389847). This can prevent the policy from learning. Consider scaling down the VF loss by reducing vf_loss_coeff, or disabling vf_share_layers.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 3 (3 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Knapsack-v0_8ff22748</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         39.7761</td><td style=\"text-align: right;\">      12000</td><td style=\"text-align: right;\"> 346.351</td></tr>\n",
       "<tr><td>PPO_Knapsack-v0_8ff5199e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         40.1578</td><td style=\"text-align: right;\">      12000</td><td style=\"text-align: right;\"> 309.028</td></tr>\n",
       "<tr><td>PPO_Knapsack-v0_8ff71dde</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">1e-06 </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         40.2784</td><td style=\"text-align: right;\">      12000</td><td style=\"text-align: right;\"> 263.016</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 18:54:43,580\tINFO tune.py:334 -- Returning an analysis object by default. You can call `analysis.trials` to retrieve a list of trials. This message will be removed in future versions of Tune.\n"
     ]
    }
   ],
   "source": [
    "def create_env(config_env):\n",
    "#     env = gym.make(config_env[\"version\"])\n",
    "    return KnapsackEnv()\n",
    "\n",
    "tf = try_import_tf()\n",
    "\n",
    "class CustomModel(TFModelV2):\n",
    "    \"\"\"Example of a custom model that just delegates to a fc-net.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name):\n",
    "        super(CustomModel, self).__init__(obs_space, action_space, num_outputs,\n",
    "                                          model_config, name)\n",
    "        self.model = FullyConnectedNetwork(obs_space, action_space,\n",
    "                                           num_outputs, model_config, name)\n",
    "        self.register_variables(self.model.variables())\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        return self.model.forward(input_dict, state, seq_lens)\n",
    "\n",
    "    def value_function(self):\n",
    "        return self.model.value_function()\n",
    "\n",
    "# Can also register the env creator function explicitly with:\n",
    "# register_env(\"Knapsack-v0\", create_env)\n",
    "# register_env(\"Knapsack-v0\", create_env)\n",
    "register_env(\"Knapsack-v0\", lambda config: create_env(config))\n",
    "ray.init(ignore_reinit_error=True)\n",
    "ModelCatalog.register_custom_model(\"my_model\", CustomModel)\n",
    "x = tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\n",
    "        \"timesteps_total\": 10000,\n",
    "    },\n",
    "    config={\n",
    "        \"env\": \"Knapsack-v0\",  # or \"corridor\" if registered above\n",
    "        \"model\": {\n",
    "            \"custom_model\": \"my_model\",\n",
    "        },\n",
    "        \"env_config\": {\n",
    "            \"version\": \"Knapsack-v0\"\n",
    "#             \"corridor_length\": 5,\n",
    "        },\n",
    "        \"vf_share_layers\": True,\n",
    "        \"lr\": grid_search([1e-2, 1e-4, 1e-6]),  # try different lrs\n",
    "        \"num_workers\": 1,  # parallelism\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_len_mean</th>\n",
       "      <th>episodes_this_iter</th>\n",
       "      <th>timesteps_this_iter</th>\n",
       "      <th>done</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>episodes_total</th>\n",
       "      <th>training_iteration</th>\n",
       "      <th>...</th>\n",
       "      <th>info/learner/default_policy/kl</th>\n",
       "      <th>info/learner/default_policy/entropy</th>\n",
       "      <th>info/learner/default_policy/entropy_coeff</th>\n",
       "      <th>config/env</th>\n",
       "      <th>config/env_config</th>\n",
       "      <th>config/lr</th>\n",
       "      <th>config/model</th>\n",
       "      <th>config/num_workers</th>\n",
       "      <th>config/vf_share_layers</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>351.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>258.828877</td>\n",
       "      <td>21.385027</td>\n",
       "      <td>187</td>\n",
       "      <td>4000</td>\n",
       "      <td>True</td>\n",
       "      <td>12000</td>\n",
       "      <td>571</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197311</td>\n",
       "      <td>2.035856</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Knapsack-v0</td>\n",
       "      <td>{'version': 'Knapsack-v0'}</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>{'custom_model': 'my_model'}</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/christian/ray_results/PPO/PPO_Knapsack-v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>511.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>304.865922</td>\n",
       "      <td>22.418994</td>\n",
       "      <td>179</td>\n",
       "      <td>4000</td>\n",
       "      <td>True</td>\n",
       "      <td>12000</td>\n",
       "      <td>556</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027375</td>\n",
       "      <td>5.177581</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Knapsack-v0</td>\n",
       "      <td>{'version': 'Knapsack-v0'}</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>{'custom_model': 'my_model'}</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/christian/ray_results/PPO/PPO_Knapsack-v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>380.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>262.000000</td>\n",
       "      <td>20.701031</td>\n",
       "      <td>194</td>\n",
       "      <td>4000</td>\n",
       "      <td>True</td>\n",
       "      <td>12000</td>\n",
       "      <td>582</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>5.294332</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Knapsack-v0</td>\n",
       "      <td>{'version': 'Knapsack-v0'}</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>{'custom_model': 'my_model'}</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/christian/ray_results/PPO/PPO_Knapsack-v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows  51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
       "0               351.0               177.0           258.828877   \n",
       "1               511.0               176.0           304.865922   \n",
       "2               380.0               134.0           262.000000   \n",
       "\n",
       "   episode_len_mean  episodes_this_iter  timesteps_this_iter  done  \\\n",
       "0         21.385027                 187                 4000  True   \n",
       "1         22.418994                 179                 4000  True   \n",
       "2         20.701031                 194                 4000  True   \n",
       "\n",
       "   timesteps_total  episodes_total  training_iteration  ...  \\\n",
       "0            12000             571                   3  ...   \n",
       "1            12000             556                   3  ...   \n",
       "2            12000             582                   3  ...   \n",
       "\n",
       "  info/learner/default_policy/kl info/learner/default_policy/entropy  \\\n",
       "0                       0.197311                            2.035856   \n",
       "1                       0.027375                            5.177581   \n",
       "2                       0.001136                            5.294332   \n",
       "\n",
       "   info/learner/default_policy/entropy_coeff   config/env  \\\n",
       "0                                        0.0  Knapsack-v0   \n",
       "1                                        0.0  Knapsack-v0   \n",
       "2                                        0.0  Knapsack-v0   \n",
       "\n",
       "            config/env_config  config/lr                  config/model  \\\n",
       "0  {'version': 'Knapsack-v0'}   0.010000  {'custom_model': 'my_model'}   \n",
       "1  {'version': 'Knapsack-v0'}   0.000100  {'custom_model': 'my_model'}   \n",
       "2  {'version': 'Knapsack-v0'}   0.000001  {'custom_model': 'my_model'}   \n",
       "\n",
       "  config/num_workers  config/vf_share_layers  \\\n",
       "0                  1                    True   \n",
       "1                  1                    True   \n",
       "2                  1                    True   \n",
       "\n",
       "                                              logdir  \n",
       "0  /home/christian/ray_results/PPO/PPO_Knapsack-v...  \n",
       "1  /home/christian/ray_results/PPO/PPO_Knapsack-v...  \n",
       "2  /home/christian/ray_results/PPO/PPO_Knapsack-v...  \n",
       "\n",
       "[3 rows x 51 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
